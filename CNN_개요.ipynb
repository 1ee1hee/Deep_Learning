{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - 합성곱 신경망(Convolution Neural Network)\n",
    "- 주로 컴퓨터 비전(이미지, 동영상 관련 처리)에서 사용되는 딥러닝 모델로 Convolution 레이어를 이용해 데이터의 특징을 추출하는 전처리 작업을 포함시킨 신경망(Neural Network) 모델\n",
    "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FVT2ok%2FbtqUejsTLzm%2Fkk9SSJuTwUDaKPHHOyhdw0%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 응용되는 다양한 컴퓨터 비전 영역\n",
    "### 1. Image Classification(이미지 분류)\n",
    "- 입력된 이미지가 어떤 라벨에 대응되는지 이미지에 대한 분류(Classification)을 처리\n",
    "\n",
    "![image](https://velog.velcdn.com/images/aischool/post/fc7fea1c-9758-4111-97b0-8cf9cd5b7eba/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Object Detection(물체 검출)\n",
    "- 이미지 안에 Object(물체)들의 위치를 찾고 어떤 물체인지 분류하는 작업을 한다\n",
    "- Localization: 이미지 안에서 하나의 Object의 위치와 class를 분류\n",
    "- Detection: 이미지안의 여러개의 Object의 위치와 class를 분류\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/b9afee71-4894-4e17-964c-c70fe7b693dc/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image Segmentation(세분화)\n",
    "- 이미지를 입력 받아 픽셀별로 분류\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/fd061605-6a09-481c-9a3d-3e4158168337/image.png)\n",
    "- Semantic segmentation(의미 기반 세분화)\n",
    "    - 클래스 단위로 구분\n",
    "    - 같은 클래스는 같은 것으로 구분\n",
    "- Instance segmantation(인스턴스 기반 세분화)\n",
    "    - 각 대체 단위로 구분\n",
    "    - 동일한 클래스라도 다른 객체일 경우 다른 것으로 구분\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/99e71baa-e890-48af-8822-6b37fd7d68be/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification, Localization, Object Detection, Segmentation 차이\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/9ff3bf15-0666-4bce-b0c1-52cd902b35e0/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Image Caotioning\n",
    "- 이미지에 대한 설명문을 자동 생성\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/b464e251-00ca-45cd-8675-afeaeb5c4094/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Super Resolution\n",
    "- 저해상도의 이미지롤 고해상도의 이미지로 변환\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/b8fd0f91-6826-405f-9b10-8ce48d1a3b95/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Neural Style Transfer\n",
    "- 입력 이미지와 스타일 이미지를 합쳐 합성된 새로운 이미지 생성\n",
    "- 이미지 스타일 변경\n",
    "    - ex) 사진을 피카소 그림 스타일로 변경\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/3e55c11f-9872-45df-ab0c-4455ee53947f/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Text Detection & OCR\n",
    "- Text Detection: 이미지 내 텍스트 영역을 Bounding Box로 찾아 표시\n",
    "- OCR: Text Detection이 처리된 Bounding Box 안 글자들이 어떤 글자인지 찾음\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/ddc84ce4-95d1-4e5b-94d0-aadca262b6eb/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Keypoint Detection(특징점 검출)\n",
    "- 인간의 특징점(Keypoint)들을 추정\n",
    "    - 풀고자 하는 문제\n",
    "        - Human Pose estimation: 사람의 관절을 검출해 자세를 추정\n",
    "        - Face keypoint detection: 사람 얼굴의 각 특징점을 추출해 사람의 표정을 추정\n",
    "        - Hand detection: 손가락 관절들을 검출해 손의 형태를 추정\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/53236de9-3839-41e7-b967-40a73cbb6c49/image.png)\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/99d96c53-6859-4d6b-b68f-92966201ebaa/image.png)\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/8cd3d332-28c5-4c57-8a53-f2e9f4c94124/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision이 어려운 이유\n",
    "- ### 사람과 컴퓨터가 보는 이미지의 차이\n",
    "    - 컴퓨터가 보는 이미지는 0 ~ 255 사이의 숫자로 이뤄진 행렬\n",
    "    - 행렬안에서 패턴을 찾는 것은 쉽지 않음 \n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/3ad37ddf-fec8-4b3d-939f-b24481e4d7c2/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 배경과 대상이 비슷해 구별이 안됨\n",
    "- 명암이나 배경에 의해 경계가 구별이 안되는 경우\n",
    "![image](https://velog.velcdn.com/images/kimjunsu97/post/c6d804dc-412f-4c15-8749-ad1414ed0db0/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 같은 종류의 대상도 형태가 너무 많음\n",
    "![image](https://velog.velcdn.com/images/kimjunsu97/post/78d2125c-f9e1-438f-b630-7f36f2f36e9a/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 대상이 가려져 있는 경우\n",
    "![image](https://velog.velcdn.com/images/kimjunsu97/post/6c58e58d-e82c-4790-b7cb-5afee0fdfeb5/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 같은 class에 다양한 형태가 있음\n",
    "![image](https://pinktree.kr/web/product/big/pf021good500.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 전통적인 이미지 처리 방삭과 딥러닝의 차이\n",
    "### Handcrafted Feature(전통적인 영상처리 방식)\n",
    "- 분류하려고 하는 이미지의 특징들을 사람이 직접 찾아서 만든다(Feature Extraction)\n",
    "    - 찾아낸 특징들을 기반으로 학습\n",
    "    - 특성 추출을 위해 Filter 행렬을 주로 이용\n",
    "- 발견하지 못한 특징을 가진 이미지에 대해서는 분류를 하지 못하기 때문에 성능이 저하\n",
    "- 다양한 많은 대상들에 대해 특징을 추출하는 것을 사람이 직접하기 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to End learning(딥러닝)\n",
    "- 이미지의 특징 추출부터 추론까지 자동으로 학습\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/c11ea4aa-3ce5-4a74-a42c-db37d699840e/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN(Convolution Neural Network)\n",
    "### 구성\n",
    "- 이미지로 부터 부분적 특성을 추출하는 **Feature Extraction** 부분과 분류를 위한 **추론 부분**으로 나눈다\n",
    "- **`Feature Extraction 부분에 이미지 특징 추출에 성능이 좋은 Convolution Layer를 사용`**\n",
    "    - Feature Extraction: Convolution layer\n",
    "    - 추론: Dense Layer(Fully connectend layer)\n",
    "    \n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/202ceed3-eb8d-4759-a16b-084a3a809652/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영상 처리에서 Feature Extractor를 Dense Layer를 사용했을 때 문제점\n",
    "- Demse layer는 이미지의 공간적 구조를 학습하는 것이 어려움\n",
    "    - 같은 형태가 전체 이미지에서 위치가 바뀌었을때 다른 값으로 인식\n",
    "- 이미지를 input으로 사용하면 weight의 양이 매우 큼\n",
    "    - weight가 많으면 학습 대상이 많아 그만큼 어려움\n",
    "    - 64 * 64 픽셀 이미지의 경우\n",
    "        - 흑백은 Unit(노드) 하나당 500 * 500 = 250000개 학습 파라미터 (가중치 - weight)\n",
    "        - 컬러는 Unit(노드) 하나당 500 * 500 * 3(RGB 3가지) = 750000개 학습 파라미터(가중치 - weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution(합성곱) 연산 이란?\n",
    "- Convolution Layer는 이미지와 필터간의 **Convolution(합성곱) 연산**을 통해 이미지의 특징을 추출\n",
    "- 합성곡 연산은 input data와 weight간 가중합을 구할 때 한번에 구하지 않고 작은 크기의 Filter를 이동시키면서 가중합을 구함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolution 연산\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/ba4b6d55-07b0-4f3b-815a-2d3867f103f0/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolution 연산\n",
    "- input 행렬과 Filter 행렬간 행렬곱 연산 수행\n",
    "    - 동일한 index의 값끼리 곱한 후 더함\n",
    "\n",
    "1.\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/a7cbaeef-e084-454b-a774-10765e057fd0/image.png)\n",
    "\n",
    "2.\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/b104f2a4-4396-4a32-9dde-b48af056d0ed/image.png)\n",
    "\n",
    "3.\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/e201620c-999d-4414-a6ee-73f43845eece/image.png)\n",
    "\n",
    "4.\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/c8de73ba-f8e3-469b-8ec9-32649c646bc6/image.png)\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/3e0ee697-6511-4ffa-9807-588f5d59a949/image.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature(특성) 추출과 합성곱\n",
    "- Filter(Kernel)\n",
    "    - 이미지와 합성곱 연산을 통해 Feature(패턴)을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How?\n",
    "- 대상 이미지\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/76e92db7-b4c5-43c1-9b48-621a5489d22d/image.png)\n",
    "\n",
    "- Filter / Kernel\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/fdfffc02-2927-438a-9749-bd5c73705524/image.png)\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/4ef94ca8-a0eb-4d89-8165-d791a79715e3/image.png)\n",
    "**필터와 이미지의 노란 박스 부분을 합성곱하면 6600이 나옴**\n",
    "\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/1dc74713-435f-433b-b1e2-df9353af114f/image.png)\n",
    "**필터와 이미지의 노란 박스 부분을 합성곱하면 0이 나옴**\n",
    "\n",
    "\n",
    "- #### **`필터와 부분 이미지의 합성곱 결과가 값이 나온다는 것은 그 부분 이미지에 필터가 표현하는 이미지 특성이 존재`**\n",
    "    - 기존 Hand craft 방식은 이미지의 특성을 잘 찾을 수 있는 Filter를 사람이 만들었다\n",
    "    - CNN은 Filter를 데이터 학습을 통해 만든다\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Craft 방식의 Filter\n",
    "- 원본 데이터\n",
    "![image](https://www.researchgate.net/profile/Hugo-Hidalgo-Silva/publication/253682881/figure/fig2/AS:298135596355602@1448092473989/Lena-Original-Image-512x512-pixels.png)\n",
    "\n",
    "- 영상으로 부터 윤곽선 특성(Edge Feature)을 찾는다\n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/ad54e47f-924c-43dc-a167-ce554dd2659f/image.png)\n",
    "\n",
    "- Sobel 필터\n",
    "    - X-Direction Kernel: 이미지에서 수평 윤곽선(edge)를 찾음\n",
    "    - Y-Direction Kernel: 이미지에서 수직 윤곽선(edge)를 찾음\n",
    "    \n",
    "![image](https://velog.velcdn.com/images/lee1066515/post/01233552-4643-47c7-bee1-86183d9a5fd6/image.png)\n",
    "- 왼쪽 : X-Direction Kernel 적용, 오른쪽: Y-Direction Kernel 적용\n",
    "\n",
    "- 영상의 윤곽선이 무제 해결에 도움이 되는 특성이면 영상으로 부터 윤곽을 추출할 수 있는 **Filter를 개발자가 연구를 통해 찾아내야 함**\n",
    "- 필요한 특성이 N개 있다면 그것들을 찾기 위한 필터 N개를 만들어 특성을 추출\n",
    "- 추출된 특성들을 머신러닝 모델의 입력으로 넣어 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning(CNN)에서의 Filter\n",
    "- Convolution Layer는 특성을 추출하는 Filter(Kernel)들로 구성\n",
    "    - **Filter(Kernel)를 구성하는 값들은 데이터를 학습해 찾아냄**\n",
    "    - 그래서 Filter를 구성하는 원소(element)들이 Parameter(weight)가 됨\n",
    "- 한 Layer를 구성하는 Filter들은 Input(input image 또는 feature map)에서 각각 다른 패턴(특징)들을 찾아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer도 여러층을 쌓는다\n",
    "- 입력층(Bottom)과 가까운 Convolution 레이어일수록 input image의 작은 영역에서의 특징들을 찾는다. 작은 영역에서의 특징을 찾기 때문에 **이미지의 엣지나 경계선등 일반화가 쉬운 이미지의 기초적인 표현을 찾음**\n",
    "    - 선, 질감 등\n",
    "- 출력층(Top)과 가까운 Convolution 레이어일수록 input image의 넓은(큰) 영역에서의 특징들을 찾음, 넓은 영역에서의 특징을 찾기 때문에 **일반화가 곤란한 구체적인 이미지의 표현을 찾음**\n",
    "    - 눈, 코, 의자, 나무, 고양이\n",
    "\n",
    "![image](https://velog.velcdn.com/images/ppippi/post/3c66b54b-90af-4b6e-a78c-acc95c560e21/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer 생성 및 작동 방식\n",
    "### tensorflow.keras.layers.Conv2D 사용\n",
    "- **Hyper parameter**\n",
    "    - **filters**:\n",
    "        - 데이터를 구성하는 filter(kernel)의 개수. Feature map output의 깊이가 된다\n",
    "    - **kernel_size**:\n",
    "        - Filter의 크기(height, weight)\n",
    "        - 보통 홀수 크기로 잡는다(3 * 3, 5 * 5). 주로 3 * 3 필터를 사용\n",
    "            - height, width가 동일한 경우 하나만 설정\n",
    "        - 필터의 채널은 Layer의 input의 채널과 동일하게 자동으로 설정\n",
    "    - padding\n",
    "        - input tensor의 외곽에 특정 값(보통 0)를 둘러 쌈\n",
    "    - stride\n",
    "        - 연산시 Filter의 이동 크기\n",
    "\n",
    "- **Feature Map**\n",
    "    - Filter를 거쳐 나온 결과물\n",
    "    - Feature map의 개수는 Filter당 한개가 생성\n",
    "    - Feature map의 크기(shape)는 Filter의 크기(shape), Stride, Padding 설정에 따라 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input shape\n",
    "- (데이터 개수, height, width, channel)\n",
    "    - Height: 세로 길이\n",
    "    - Width: 가로 길이\n",
    "    - Channel: 하나의 data를 구성하는 행렬의 개수\n",
    "        - 이미지: 색성분\n",
    "            - 흑백(Gray scale) 이미지는 하나의 행렬로 구성\n",
    "            - 컬러 이미지는 RGB의 각 이미지로 구성되어 3개의 행렬로 구성\n",
    "        - Feature map: 특성개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 추출 연산\n",
    "![image](https://velog.velcdn.com/images/ppippi/post/067a5e13-5ef0-465b-83b6-b023a59b577f/image.png)\n",
    "\n",
    "- **예1**\n",
    "    - Input image는  $6 \\times 6 \\times 3$ 형태 (높이, 너비, 채널)\n",
    "    - Filter: $3 \\times 3 \\times 3$ 크기의 필터 1개   (높이, 너비, 채널)\n",
    "    - Output: $4 \\times 4$  feature map 1개 생성 \n",
    "\n",
    "![imaeg](https://mblogthumb-phinf.pstatic.net/MjAyMDA1MjhfOTcg/MDAxNTkwNjUxNjI2OTY1.AAjGX1ixSwQr1D1RmN2qNzN_M2LiZdIkN_CdSynBVC8g.tPc5_erL4Eejwgi_YzTHj712ZD9963ME7Y6QaeN60FIg.PNG.jk96491/image.png?type=w800)\n",
    "\n",
    "- **예2**\n",
    "    - Input image는  $6 \\times 6 \\times 3$ 형태의 volume\n",
    "    - Filter: $3 \\times 3 \\times 3$ 크기의 필터 2개\n",
    "    - Output: $4 \\times 4 $  feature map 2개 생성 \n",
    "\n",
    "![image](https://velog.velcdn.com/images/coral2cola/post/aca75a49-4135-4ca5-ab3f-58305c6a2f52/image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "- 이미지 가장자리의 픽셀은 convolution 계산에 상대적으로 적게 반영\n",
    "- 이미지 가장자리를 0으로 둘러싸서 가장자리 픽셀에 대한 반영 횟수를 늘림\n",
    "    - 0으로 둘러싸는 것을 ZeroPadding이라고 함\n",
    "- Padding을 이용해 Feature map의 size를 조절 가능\n",
    "- Conv2D의 padding 속성은 다음 두가지 옵션으로 설정\n",
    "    - \"valid\" padding\n",
    "        - Padding을 적용하지 않음\n",
    "        - Output(Feature map)의 크기가 줄어듬\n",
    "    - \"same\" padding\n",
    "        - Input과 Output의 이미지 크기가 동일하게 되도록 padding 수를 결정\n",
    "        - **보통 same 패딩을 사용**\n",
    "        - Output의 크기는 Pooling Layer를 이용해 줄임\n",
    "\n",
    "![imaeg](https://cdn-images-1.medium.com/max/1200/1*1okwhewf5KCtIPaFib4XaA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strides\n",
    "- Filter(Kernel)가 한번 Convolution 연산을 수행한 후 옆 혹은 아래로 얼마나 이동할 것인가를 설정\n",
    "- (가로이동크기, 세로이동크기)를 지정하는데 둘이 같은 경우 하나만 지정\n",
    "    - 예) stride = 2: 한 번에 두 칸씩 이동(feature map의 너비와 높이가 2배수로 다운 샘플링 되었음을 의미)\n",
    "- convolution layer에서는 일반적으로 1을 지정\n",
    "- stride = (2, 2)\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1200/1*BMngs93_rm2_BpJFH2mS0Q.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고\n",
    "- W: input의 width\n",
    "- H: input의 height\n",
    "- F: filter size\n",
    "- S: strides\n",
    "- P: the number of zero_paddings\n",
    "- EX) 32 X 32 X 3 input, 5 X 5 filter 10개, 1 stride, 0 pad => (28 X 28 X 10)\n",
    "- EX) 32 X 32 X 3 input, 3 X 3 filter 10개, 1 stride, 1 pad => (32 X 32 X 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.0, 28.0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map_width_ex1 = (32-5+2*0)/1+1\n",
    "feature_map_height_ex1 = (32-5+2*0)/1+1\n",
    "feature_map_width_ex1, feature_map_height_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.0, 32.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map_width_ex2 = (32-3+2*1)/1+1\n",
    "feature_map_height_ex2 = (32-3+2*1)/1+1\n",
    "feature_map_width_ex2, feature_map_height_ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer\n",
    "- 해당 영역의 input 중 가장 큰 값을 출력\n",
    "- 일반적으로 2 * 2 크기에 stride는 2를 사용\n",
    "- 강제적인 downsampling 효과\n",
    "    - Input의 size를 줄여 계산속도를 높임\n",
    "    - 특징의 공간적 계층구조를 학습 => 부분적 특징을 묶어 전체적인 특징의 정보를 표현\n",
    "- 학습할 weight가 없음: 일반적으로 convolutional layer + pooling layer를 하나의 레이터로 취급\n",
    "\n",
    "![image](https://velog.velcdn.com/images/ppippi/post/f0e91bde-87e5-4bcc-9c62-13ebb4955408/image.png)\n",
    "\n",
    "Max Pooling layer(2 X 2 pooling kernel, stride 2, no padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론기\n",
    "- 해결하려는 문제에 맞춰 Layer를 추가\n",
    "    - Feature Extraction layer을 통과해 나온 Feature map을 입력으로 받아 추론한 최종결과를 출력\n",
    "- Dense layer를 많이 사용했으나 지금은 Convolution layer를 사용하기도 함\n",
    "    - Feature Extractor와 추론 layer를 모두 Convolution Layer로 구성한 Network를 Fully Convolution Network(FCN)이라고 한다\n",
    "\n",
    "![image](https://velog.velcdn.com/images/minjung00/post/b009c0dd-7d08-4091-b40a-f66bad462509/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CNN architecture\n",
    "![image](https://velog.velcdn.com/images/kyungmin1029/post/89a0b341-84f9-4f0f-bac0-bbb2de0966a5/image.png)\n",
    "\n",
    "![image](https://www.researchgate.net/profile/Simranjeet-Singh-7/publication/341671113/figure/fig21/AS:895930844839938@1590617967486/Fully-connected-layer-18.ppm)\n",
    "\n",
    "- 일반적으로 convolutional layer + pooling layer 구조를 여러 개 쌓는다\n",
    "    - 동일한 레이어들의 구조를 반복해서 쌓을 때 그 구조를 **Layer block**이라고 함\n",
    "    - convolution과 pooling layer를 묶어서 반복한 것을 **convolution block**이라 함\n",
    "- Bottom단의 Convolution block에서 Top단의 convolution block으로 진행될 수록 **feature map**의 size(height, width)는 작아지고 channel(depth)는 증가\n",
    "    - Top 단으로 갈수록 다른 convolution layer들과 동일한 size의 filter를 이용해 입력 이미지의 더 큰(넓은) 영역 특성을 찾아야 하기 때문에 size를 줄임\n",
    "    - Top 단으로 갈수록 더 큰 영역에서 특성을 찾게 되므로 Filter의 개수를 늘려 특성을 찾도록 함\n",
    "- 마지막에 Fully connected layer를 이용해 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras CNN 구성 Layer API\n",
    "- Input shape: (image_height, image_width, image_channels)로 지정\n",
    "- Convolution 레이어: **tensorflow.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding = 'valid', activation = None)**\n",
    "    - `filters`: the dimensionality of the output space(i.e. the number of output filters)\n",
    "    - `kernel_size`: height and width of the 2D convolution window\n",
    "    - `strides`: the strides of the convolution along the height and width\n",
    "    - `padding`: \"valid\" or \"same\"\n",
    "    - `activation`\" activation function\n",
    "\n",
    "- MaxPooling 레이어: **tensorflow.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = None, padding = 'valid')**\n",
    "    - `pool_size`: Pooling window size\n",
    "    - `strides`: default = `pool_size`\n",
    "    - `padding`: \"valid\" or \"same\"\n",
    "\n",
    "- **tensorflow.keras.layers.Flatten()**\n",
    "    - N차원 입력을 1차원 Vector로 변환\n",
    "    - Convolution Layer와 Dense Layer 사이에 연결 Layer로 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
