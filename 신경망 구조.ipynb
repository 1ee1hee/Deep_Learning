{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN(Deep Neural Network)\n",
    "## 신경망 구성요소\n",
    "### Train(학습) 프로세스\n",
    "![image](https://tensorflowkorea.files.wordpress.com/2018/12/093.jpg)\n",
    "\n",
    "### 학습 단계 및 역할\n",
    "1. 모델: 추론\n",
    "    - batch size 단위로 추론한다\n",
    "2. 손실함수(Loss Function): 모델이 추론한 것과 정답간의 Loss(손실-오차)의 평균을 구한다\n",
    "3. 옵티마이저(Optimizer): 손실함수가 구한 Loss를 기반으로 모델의 파라미터들(weight)을 업데이트한다\n",
    "- **층(Layer)**: 모델이 추론하기 위한 각 단계를 정의\n",
    "- **손실함수(loss function)**: 모델의 예측결과와 Ground Truth(실제 정답)사이의 차이를 계산\n",
    "- **optimizer**: 파라미터(가중치)를 업데이트하여 모델의 성능을 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유닛/노드/뉴런(Unit, Node, Neuron)\n",
    "- 입력 Feature들을 입력받아 처리 후 출력하는 데이터 처리 모듈\n",
    "    - Input -> Output\n",
    "- 입력 값에 Weight(가중치)를 곱하고 bias(편향)을 더한 결과를 Activation함수에 넣어 최종 결과를 출력하는 계산을 처리\n",
    "![image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*MXBl6qooI20S3rJIkyZxVA.png)\n",
    "    - **Input vector(입력값)**: $\\large\\mathbb x=(x_1, x_2, x_3)^T$\n",
    "    - **Weights(가중치)**: $\\large\\mathbb w = (w_1, w_2, w_3)^T$\n",
    "    - **Bias(편향)**: $\\large b \\in \\mathbb R$\n",
    "    - **Activation function(활성함수)**: $\\large\\sigma(\\cdot)$\n",
    "        - 선형결합한 결과를 비선형화 시키는 목적\n",
    "        - 다양한 비선형 함수들을 사용한다.\n",
    "        \n",
    "$\\LARGE z=w_1 x_1 + w_2 x_2 + w_3 x_3 + b \\Leftrightarrow z = \\mathbb w^T \\mathbb x + b$\n",
    "$\\LARGE a=\\sigma( z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이터/층(Layer)\n",
    "- 네트워크 모델의 각 처리단계를 정의한 것\n",
    "    - 실제 처리를 담당하는 Unit들을 모아놓은 구조\n",
    "\n",
    "- **Input Layer(입력층)**\n",
    "    - 입력값들을 받아 Hidden Layer에 전달하는 노드들로 구성된 Layer\n",
    "    - 입력 데이터의 shape을 설정하는 역할을 한다\n",
    "\n",
    "- **Output Layer(출력층)**\n",
    "    - 모델의 최종 예측결과를 출력하는 노드들로 구성된 Layer\n",
    "\n",
    "- **Hidden Layer(은닉층)**\n",
    "    - Input Layer와 Output Layer사이에 존재하는 Layer\n",
    "\n",
    "- 대부분 Layer들은 학습을 통해 최적화할 Parameter를 가짐\n",
    "    - Dropout, Pooling Layer와 같이 Parameter가 없는 Layer도 있다\n",
    "\n",
    "- Layer들의 연결한 것을 **<font size=5>Network</font>** 라고 한다\n",
    "    - 딥러닝은 Layer들을 깊게 쌓은 것을 말한다(여러 Layer들을 연결한 것)\n",
    "\n",
    "- 목적, 구현 방식에 다라 다양한 종류의 Layer들이 있다\n",
    "    - **Fully Connected Layer(Dense layer)**\n",
    "        - 추론 단계에서 주로 사용\n",
    "    - **Convolution Layer**\n",
    "        - 이미지 Feature extraction으로 주로 사용\n",
    "    - **Recurrent Layer**\n",
    "        - Sequential(순차)데이터의 Feature extraction으로 주로 사용\n",
    "    - **Embedding Layer**\n",
    "        - Text 데이터의 Feature extraction으로 주로 사용\n",
    "        \n",
    "- **API** \n",
    "    - https://keras.io/api/layers/\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델(Network)\n",
    "- Layer를 연결한 것이 Deep Learning 모델이다\n",
    "- 이전 레이어의 출력을 input으로 받아 처리 후 output으로 출력하는 레이어들을 연결한다\n",
    "- 적절한 network 구조(architecture)를 찾는 것은 공학적이기 보다는 경험적(Art)접근이 필요하다\n",
    "![image](https://velog.velcdn.com/images/ppippi/post/8dd46d62-f23e-46e1-b6e7-4396095cb1c4/image.png)\n",
    "- **API**\n",
    "    - https://keras.io/api/models/\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성 함수(Activation Function)\n",
    "- 각 유닛이 입력과 Weight간에 가중합을 구한 뒤 출력결과를 만들기 위해 거치는 함수\n",
    "- 같은 층(layer)의 모든 유닛들은 같은 활성 함수를 가진다\n",
    "- **출력 레이어**의 경우 출력하려는 문제에 맞춰 활성함수를 결정한다.\n",
    "- **은닉층(Hidden Layer)** 의 경우 비선형성을 주어 각 Layer가 처리하는 일을 분리하는 것을 목적으로 한다\n",
    "    - 비선형함수를 사용하지지 않으면 Layer들을 여러개 추가하는 의미가 없어진다\n",
    "    - **ReLU** 함수를 주로 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 활성함수(Activation Function)\n",
    "- ### Sigmoid(logistic function)\n",
    "    ![image](https://velog.velcdn.com/images/minjung00/post/9a2c5992-8f8c-40dc-92ef-327b9d985ee4/image.png)\n",
    "\n",
    "    - $$\\large \\sigma(z) = \\frac 1 {1+e^{-z}}$$\n",
    "    - 출력값의 범위\n",
    "        - $0<sigmoid(z)<1$\n",
    "    - 한계\n",
    "        - 초기 딥러닝의 hidden layer(은닉층)의 activation function(활성함수)로 많이 사용 되었다.\n",
    "        - 층을 깊게 쌓을 경우 **기울기 소실(Gradient Vanishing)** 문제를 발생시켜 학습이 안되는 문제가 있다.\n",
    "    - **Binary classification(이진 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 사용된다.**\n",
    "        - 모델이 positive(1)의 확률을 출력결과로 추론하도록 할 경우 사용. (unit개수 1, activation함수 sigmoid)\n",
    "        - 위와 같은 한계때문에 hidden layer(은닉층)의 activation function(활성함수)로는 잘 사용되지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **기울기 소실(Gradient Vanishing)** 문제란?\n",
    "> - 최적화 과정에서 gradient가 0이 되어 Bottom Layer의 가중치들이 학습이 안되는 현상\n",
    ">   - Bottom Layer: Input Layer(입력층)에 가까이 있는 Layer들\n",
    ">   - Top Layer: Output Layer(출력층)에 가까이 있는 Layer들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Hyperbolic tangent\n",
    "    ![image](https://velog.velcdn.com/images/minjung00/post/eb93b227-a022-4489-9f1c-9693b2137a3f/image.png)\n",
    "    - $$\\large  tanh(z) = \\cfrac{e^{z} - e^{-z}}{{e^{z} + e^{-z}}}$$\n",
    "    - 출력값의 범위\n",
    "        - $-1<tanh(z)<1$\n",
    "    - Output이 0을 중심으로 분포하므로 sigmoid보다 학습에 효율 적이다.\n",
    "    - 기울기 소실(Gradient Vanishing) 문제를 발생시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### ReLU(Rectified Linear Unit)\n",
    "    ![image](https://velog.velcdn.com/images/minjung00/post/2df77932-f75b-4087-b83d-1a9d11a82e9c/image.png)\n",
    "    $$\\large  ReLU(z)=max(0,z)$$\n",
    "    - 기울기 소실(Gradient Vanishing) 문제를 어느정도 해결\n",
    "    - 0이하의 값(z <= 0)들에 대해 뉴런이 죽는 단점이 있다. (Dying ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Leaky ReLU\n",
    "    ![image](https://velog.velcdn.com/images/minjung00/post/f8ff3726-79dc-4c9e-9a00-99e65caa7eec/image.png)\n",
    "    <br><br>\n",
    "     $$\\large  LeakyReLU(z)=max(\\alpha z,z)\\\\\n",
    "     0< \\alpha <1$$\n",
    "    - ReLU의 Dying ReLU 현상을 해결하기 위해 나온 함수\n",
    "    - 음수 z를 0으로 반환하지 않고 alpah (0 ~ 1 사이 실수)를 곱해 반환한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Softmax\n",
    "    $$\\large  Softmax(z_j) = \\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)}\\\\ \\:j=1,2, \\ldots, K$$\n",
    "    - **Multi-class classification(다중 분류)를 위한 네트워크의 Output layer(출력층)의 활성함수로 주로 사용된다.** \n",
    "        - 은닉층의 활성함수로 사용하지 않는다.\n",
    "    - Layer의 unit들의 출력 값들을 정규화 하여 각 class에 대한 확률값으로 변환한다.\n",
    "        - 출력노드들의 값은 0 ~ 1사이의 실수로 변환되고 그 값들의 총합은 1이 된다.\n",
    "- **API**\n",
    "    - https://keras.io/api/layers/activations/\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
